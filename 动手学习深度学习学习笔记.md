# 遇到的问题和解决办法

---

1.由于我之前已经安装anaconda并且安装完python和pytorch，但是在此处直接运行pip install d2l==0.17.6却遇到了问题。原因是我的python版本过高，但是又因为conda中的python无法降级，此外更新numpy也行不通，所以在此我又重新按照手册的方法继续：使用下面的命令创建一个新的环境：
conda create --name d2l python=3.9 ，rd
现在激活 d2l境：

conda activa。再接着把pytorch进行安装，我的电脑的安装代码为：pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu126。

# 1. d2l

---
配置好相应的pytorch和gpu之后，在anaconda中的d2l的环境里cd到李沐的深度学习的这个文件夹下然后打开jupyter notebook然后可以进行在jupyter notebook中进行运行代码等便于学习。

# 2.第二章：预备知识

---

1.数据操作：

（1）张量，如x.shape，查看张量个数x.numel() 

（2）运算符：包括加减乘除等等 

（3）广播机制：如a = torch.arange(3).reshape((3, 1))，b = torch.arange(2).reshape((1, 2)) 

（4）索引和切片：选定范围 

（5）节省内存。
 
2.数据预处理：

（1）读取数据：以csv文件为例，导入pandas包并调用read_csv函数。

（2）处理缺失值：可以使用 dummy_na=True来解决。 

（3）转换张量格式。

3.线性代数：

（1）标量：x = torch.tensor(3.0)此时x为一个表量可以进行相应的算术运算。 

（2）向量：x = torch.arange(4)即为一个向量。 

（3）长度：len(x)，维度，形状。 

（4）形状：A = torch.arange(20).reshape(5, 4) ，此时A为一个矩阵。 

（5）张量的运算：B = A.clone()  # 通过分配新内存，将A的一个副本分配给B，A * B为矩阵相乘的乘积，

（6）降维： A_sum_axis0 = A.sum(axis=0) A_sum_axis0, A_sum_axis0.shape，其中axis即时需要降的维度。

4.微积分

5.自动微分：

（1）自动计算导数：x.requires_grad_(True)  # 等价于x=torch.arange(4.0,requires_grad=True) x.grad。 

（2）非标量变量反向传播：当y不是标量时，向量y关于向量x的导数的最自然解释是一个矩阵。 对于高阶和高维的y和x，求导的结果可以是一个高阶张量。

（3）分离计算：将某些计算移动到记录的计算图之外。

6.概率：

（1）基本概率论：取样和随机变量 

（2）处理多个随机变量：联合概率，条件概率，贝叶斯定理

（3）期望和方差

7.查阅文档：

（1）查找模块中的所有函数和类：import torch print(dir(torch.distributions))  

（2）查找特定函数和类的用法：help(torch.ones) 

# 3.第三章：线性神经网络

---

1.线性回归：

（1）线性模型：线性假设是指目标可以表示为特征的加权和，机器学习中通常使用的是高维数据集，建模时采用线性代数表示法会比较方便。

（2）损失函数：拟合程度的度量，量化目标的实际值与预测值之间的差距。

（3）解析解：线性回归的解可以用一个公式简单地表达出来， 这类解叫作解析解。

（4）随机梯度下降：优化所有深度学习模型。 它通过不断地在损失函数递减的方向上更新参数来降低误差。最简单的用法是计算损失函数（数据集中所有样本的损失均值） 关于模型参数的导数（可以称为梯度）。

（5）矢量化加速：不适应for循环而是矢量化代码通常会带来数量级的加速。

2.softmax回归：

（1）分类问题：进行标签分类

（2）网络架构：单层神经网络。

（3）softmax运算：优化参数以最大化观测数据的概率。 

（4）小批量样本的矢量化：对小批量样本的数据执行矢量计算。 

（5）交叉熵损失。

3.图像分类数据集：

（1）读取数据集：mnist_train = torchvision.datasets.FashionMNIST(root="../data",train=True, transform=trans, download=True)，mnist_test = torchvision.datasets.FashionMNIST( root="../data", train=False, transform=trans, download=True)

# 4.第四章多层感知机

---

1.多层感知机：

（1）隐藏层：在输入层和输出层之间，处理更普遍的函数关系模型。 

（2）激活函数：通过计算加权和并加上偏置来确定神经元是否应该被激活。如relu函数，sigmoid函数，tanh函数

2.模型选择、欠拟合和过拟合：

（1)训练误差和泛化误差:训练误差（training error）是指， 模型在训练数据集上计算得到的误差。 泛化误差（generalization error）是指， 模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望。

(2)模型选择:评估几个候选模型后选择最终的模型。 

(3)验证集和测试集 

（4）欠拟合还是过拟合？由于训练和验证误差之间的泛化误差很小， 我们有理由相信可以用一个更复杂的模型降低训练误差。 这种现象被称为欠拟合（underfitting）。当我们的训练误差明显低于验证误差时要小心， 这表明严重的过拟合（overfitting）

3.权重衰减：主要是使用正则化的方式来进行权重衰减（1）暂退法：也称dropout即是正则化。

4.前向传播、反向传播和计算图：

（1）前向传播（forward propagation或forward pass） 指的是：按顺序（从输入层到输出层）计算和存储神经网络中每层的结果。

（2）反向传播（backward propagation或backpropagation）指的是计算神经网络参数梯度的方法。 简言之，该方法根据微积分中的链式规则，按相反的顺序从输出层到输入层遍历网络。

5.数值稳定性和模型初始化：

（1）梯度消失和梯度爆炸：不稳定的梯度会导致梯度消失或者梯度爆炸（误差变得非常大）。要么完全激活要么完全不激活（就像生物神经元）是导致梯度消失问题的一个常见的原因。当sigmoid函数的输入很大或是很小时，它的梯度都会消失。

6.环境和分布偏移：模型表现得非常出色。 但是当数据分布突然改变时，模型在部署中会出现灾难性的失败。

# 5.第五章：深度学习计算

---

1.层和块：

（1）自定义块：通过继承nn.Module类自定义神经网络块，需实现__init__（定义层）和forward（定义计算逻辑）方法。

（2）顺序块：使用nn.Sequential容器按顺序组合多个层，简化前向传播流程。

（3）前向传播中执行代码：可在forward方法中加入控制流（如条件语句）实现动态计算图。

（4）效率：注意层间数据传递的内存占用，避免不必要的张量复制。

2 参数管理：

（1）参数访问：通过named_parameters()获取参数名称和值，parameters()获取所有参数。

（2）参数初始化：使用init模块（如nn.init.xavier_uniform_()）初始化权重，防止梯度消失/爆炸。

（3）参数绑定：共享层间参数（如编码器-解码器共享词嵌入矩阵）。

3 延后初始化：

（1）实例化网络：定义网络时可不指定输入维度，首次前向传播时自动推断参数形状。

4 自定义层：

（1）不带参数的层：如nn.ReLU()，仅实现计算逻辑。

（2）带参数的层：如自定义卷积层，需在__init__中声明可训练参数。

5 读写文件：

（1）张量存储：torch.save()/torch.load()保存/加载张量。

（2）模型存储：保存model.state_dict()而非整个模型，确保代码兼容性。

6 GPU加速：



（1）设备管理：torch.device('cuda')指定GPU设备。

（2）张量迁移：tensor.to(device)将数据移至GPU。

（3）模型迁移：model.to(device)将整个模型移至GPU。

# 6.第六章：卷积神经网络

---

1 核心概念：

（1)平移不变性：卷积核在图像不同位置提取相同特征（如边缘检测）。

(2)局部性：卷积核仅关注局部区域，忽略遥远像素的关系。

2 图像卷积:

(1)互相关运算：实际执行的滑动窗口乘加操作（非数学卷积）。

(2)卷积层作用：通过可学习滤波器提取特征（如纹理、形状）。

3 填充与步幅:

(1)填充：在图像边缘补零，控制输出尺寸（如保持输入输出同尺寸）。

(2)步幅：卷积核移动步长，减少输出尺寸（下采样）。

4 多通道机制:

(1)多输入通道：每个输入通道对应独立卷积核，结果求和。

(2)多输出通道：每组卷积核生成一个输出通道，扩展特征维度。

(3)1×1卷积：用于通道间信息融合，等效于全连接层。

5 汇聚层（池化）:

(1)最大池化：提取局部最显著特征，增强平移鲁棒性。

(2)平均池化：平滑特征响应，减少背景噪声。

6 LeNet实战:

(1)网络结构：卷积层→池化层→卷积层→池化层→全连接层。

(2)训练技巧：使用交叉熵损失和SGD优化器，应用于手写数字识别。

# 7.第七章：现代卷积神经网络

---

1 AlexNet:

(1)关键创新：ReLU激活函数、Dropout正则化、多GPU训练。

(2)ImageNet表现：2012年ImageNet冠军，Top-5错误率15.3%。

2 VGG:

(1)核心思想：重复使用简单块（3×3卷积+池化）构建深层网络。

(2)VGG块：多个3×3卷积堆叠，增大感受野（等效于更大卷积核）。

3 NiN（网络中的网络）:

(1)1×1卷积应用：替代全连接层，减少参数量。

(2)全局平均池化：直接生成类别概率，避免全连接层过拟合。

4 GoogLeNet:

(1)Inception块：并行多尺度卷积（1×1, 3×3, 5×5）融合特征。

(2)辅助分类器：中间层添加分类损失，缓解梯度消失。

5 批量规范化:

(1)原理：标准化层输入，加速训练并提升泛化能力。

(2)效果：允许使用更大学习率，减少对初始化的敏感度。

6 ResNet（残差网络）:

(1)残差块：引入跳跃连接（恒等映射），解决深层网络退化问题。

(2)残差学习：拟合目标变为F(x) = H(x) - x，而非直接拟合H(x)。

7 DenseNet（稠密连接）:

(1)稠密块：每层输入来自前面所有层的输出，特征重用最大化。

(2)过渡层：控制特征图数量，连接不同稠密块。

# 8.第八章：循环神经网络

---

1 序列模型基础:

(1)自回归模型：使用历史数据预测未来（如时间序列预测）。

(2)马尔可夫假设：当前状态仅依赖前τ个时间步。

2 文本预处理:

(1)词元化：分词（如BPE算法）或字符级切分。

(2)词表构建：建立词元到索引的映射，处理低频词（如<unk>）。

3 语言模型:

(1)n元语法：基于n-1阶马尔可夫假设的统计模型（如bigram）。

(2)长序列处理：随机采样或顺序分区生成训练样本。

4 RNN核心机制:

(1)隐状态：压缩历史信息的向量，在时间步间传递。

(2)循环层计算：hₜ = f(hₜ₋₁, xₜ)，如tanh(Wₕₕhₜ₋₁ + Wₕₓxₜ)。

(3)困惑度：评价语言模型性能，低困惑度表示预测更准。

5 RNN实现:

(1)独热编码：将词元转换为稀疏向量。

(2)梯度裁剪：防止梯度爆炸，限制梯度范数（如torch.nn.utils.clip_grad_norm_）。

# 9.第九章：现代循环神经网络

---

1 GRU（门控循环单元）:

（1）重置门：控制历史信息遗忘程度。

（2）更新门：平衡新输入与历史信息的比例。

2 LSTM（长短期记忆）:

(1)记忆元：存储长期信息的水平线。

(2)三门机制：输入门：控制新信息写入;遗忘门：控制历史信息遗忘;输出门：控制信息读出

3 深度RNN:

(1)堆叠结构：多个循环层堆叠，提取更高阶时序特征。

(2)跳跃连接：缓解深层网络优化困难。

4 双向RNN:

(1)上下文捕捉：同时利用过去和未来信息（如完形填空任务）。

(2)限制：不能用于实时预测（需完整序列输入）。

5 编码器-解码器架构:

(1)编码器：将输入序列压缩为上下文向量（如LSTM最后隐状态）。

(2)解码器：基于上下文向量生成输出序列（如机器翻译）。

6 Seq2Seq模型:

(1)教师强制：训练时将真实标签作为解码器输入，加速收敛。

(2)BLEU评分：评价生成文本与参考文本的相似度。

7 束搜索:

(1)贪心搜索缺点：局部最优导致全局次优。

(2)束搜索：保留Top-k候选序列，平衡质量与计算开销。