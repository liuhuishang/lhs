# 遇到的问题和解决办法

---

1.由于我之前已经安装anaconda并且安装完python和pytorch，但是在此处直接运行pip install d2l==0.17.6却遇到了问题。原因是我的python版本过高，但是又因为conda中的python无法降级，此外更新numpy也行不通，所以在此我又重新按照手册的方法继续：使用下面的命令创建一个新的环境：
conda create --name d2l python=3.9 ，rd
现在激活 d2l境：

conda activa。再接着把pytorch进行安装，我的电脑的安装代码为：pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 --index-url https://download.pytorch.org/whl/cu126。

# 1. d2l

---
配置好相应的pytorch和gpu之后，在anaconda中的d2l的环境里cd到李沐的深度学习的这个文件夹下然后打开jupyter notebook然后可以进行在jupyter notebook中进行运行代码等便于学习。

# 2.第二章：预备知识

---

1.数据操作：

（1）张量，如x.shape，查看张量个数x.numel() 

（2）运算符：包括加减乘除等等 

（3）广播机制：如a = torch.arange(3).reshape((3, 1))，b = torch.arange(2).reshape((1, 2)) 

（4）索引和切片：选定范围 

（5）节省内存。
 
2.数据预处理：

（1）读取数据：以csv文件为例，导入pandas包并调用read_csv函数。

（2）处理缺失值：可以使用 dummy_na=True来解决。 

（3）转换张量格式。

3.线性代数：

（1）标量：x = torch.tensor(3.0)此时x为一个表量可以进行相应的算术运算。 

（2）向量：x = torch.arange(4)即为一个向量。 

（3）长度：len(x)，维度，形状。 

（4）形状：A = torch.arange(20).reshape(5, 4) ，此时A为一个矩阵。 

（5）张量的运算：B = A.clone()  # 通过分配新内存，将A的一个副本分配给B，A * B为矩阵相乘的乘积，

（6）降维： A_sum_axis0 = A.sum(axis=0) A_sum_axis0, A_sum_axis0.shape，其中axis即时需要降的维度。

4.微积分

5.自动微分：

（1）自动计算导数：x.requires_grad_(True)  # 等价于x=torch.arange(4.0,requires_grad=True) x.grad。 

（2）非标量变量反向传播：当y不是标量时，向量y关于向量x的导数的最自然解释是一个矩阵。 对于高阶和高维的y和x，求导的结果可以是一个高阶张量。

（3）分离计算：将某些计算移动到记录的计算图之外。

6.概率：

（1）基本概率论：取样和随机变量 

（2）处理多个随机变量：联合概率，条件概率，贝叶斯定理

（3）期望和方差

7.查阅文档：

（1）查找模块中的所有函数和类：import torch print(dir(torch.distributions))  

（2）查找特定函数和类的用法：help(torch.ones) 

# 3.第三章：线性神经网络

---

1.线性回归：

（1）线性模型：线性假设是指目标可以表示为特征的加权和，机器学习中通常使用的是高维数据集，建模时采用线性代数表示法会比较方便。

（2）损失函数：拟合程度的度量，量化目标的实际值与预测值之间的差距。

（3）解析解：线性回归的解可以用一个公式简单地表达出来， 这类解叫作解析解。

（4）随机梯度下降：优化所有深度学习模型。 它通过不断地在损失函数递减的方向上更新参数来降低误差。最简单的用法是计算损失函数（数据集中所有样本的损失均值） 关于模型参数的导数（可以称为梯度）。

（5）矢量化加速：不适应for循环而是矢量化代码通常会带来数量级的加速。

2.softmax回归：

（1）分类问题：进行标签分类

（2）网络架构：单层神经网络。

（3）softmax运算：优化参数以最大化观测数据的概率。 

（4）小批量样本的矢量化：对小批量样本的数据执行矢量计算。 

（5）交叉熵损失。

3.图像分类数据集：

（1）读取数据集：mnist_train = torchvision.datasets.FashionMNIST(root="../data",train=True, transform=trans, download=True)，mnist_test = torchvision.datasets.FashionMNIST( root="../data", train=False, transform=trans, download=True)

# 4.第四章多层感知机

---

1.多层感知机：

（1）隐藏层：在输入层和输出层之间，处理更普遍的函数关系模型。 

（2）激活函数：通过计算加权和并加上偏置来确定神经元是否应该被激活。如relu函数，sigmoid函数，tanh函数

2.模型选择、欠拟合和过拟合：

（1)训练误差和泛化误差:训练误差（training error）是指， 模型在训练数据集上计算得到的误差。 泛化误差（generalization error）是指， 模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望。

(2)模型选择:评估几个候选模型后选择最终的模型。 

(3)验证集和测试集 

（4）欠拟合还是过拟合？由于训练和验证误差之间的泛化误差很小， 我们有理由相信可以用一个更复杂的模型降低训练误差。 这种现象被称为欠拟合（underfitting）。当我们的训练误差明显低于验证误差时要小心， 这表明严重的过拟合（overfitting）

3.权重衰减：主要是使用正则化的方式来进行权重衰减（1）暂退法：也称dropout即是正则化。

4.前向传播、反向传播和计算图：

（1）前向传播（forward propagation或forward pass） 指的是：按顺序（从输入层到输出层）计算和存储神经网络中每层的结果。

（2）反向传播（backward propagation或backpropagation）指的是计算神经网络参数梯度的方法。 简言之，该方法根据微积分中的链式规则，按相反的顺序从输出层到输入层遍历网络。

5.数值稳定性和模型初始化：

（1）梯度消失和梯度爆炸：不稳定的梯度会导致梯度消失或者梯度爆炸（误差变得非常大）。要么完全激活要么完全不激活（就像生物神经元）是导致梯度消失问题的一个常见的原因。当sigmoid函数的输入很大或是很小时，它的梯度都会消失。

6.环境和分布偏移：模型表现得非常出色。 但是当数据分布突然改变时，模型在部署中会出现灾难性的失败。

# 5.第五章：深度学习计算

---

1.层和块：

（1）自定义块：通过继承nn.Module类自定义神经网络块，需实现__init__（定义层）和forward（定义计算逻辑）方法。

（2）顺序块：使用nn.Sequential容器按顺序组合多个层，简化前向传播流程。

（3）前向传播中执行代码：可在forward方法中加入控制流（如条件语句）实现动态计算图。

（4）效率：注意层间数据传递的内存占用，避免不必要的张量复制。

2 参数管理：

（1）参数访问：通过named_parameters()获取参数名称和值，parameters()获取所有参数。

（2）参数初始化：使用init模块（如nn.init.xavier_uniform_()）初始化权重，防止梯度消失/爆炸。

（3）参数绑定：共享层间参数（如编码器-解码器共享词嵌入矩阵）。

3 延后初始化：

（1）实例化网络：定义网络时可不指定输入维度，首次前向传播时自动推断参数形状。

4 自定义层：

（1）不带参数的层：如nn.ReLU()，仅实现计算逻辑。

（2）带参数的层：如自定义卷积层，需在__init__中声明可训练参数。

5 读写文件：

（1）张量存储：torch.save()/torch.load()保存/加载张量。

（2）模型存储：保存model.state_dict()而非整个模型，确保代码兼容性。

6 GPU加速：



（1）设备管理：torch.device('cuda')指定GPU设备。

（2）张量迁移：tensor.to(device)将数据移至GPU。

（3）模型迁移：model.to(device)将整个模型移至GPU。

# 6.第六章：卷积神经网络

---

1 核心概念：

（1)平移不变性：卷积核在图像不同位置提取相同特征（如边缘检测）。

(2)局部性：卷积核仅关注局部区域，忽略遥远像素的关系。

2 图像卷积:

(1)互相关运算：实际执行的滑动窗口乘加操作（非数学卷积）。

(2)卷积层作用：通过可学习滤波器提取特征（如纹理、形状）。

3 填充与步幅:

(1)填充：在图像边缘补零，控制输出尺寸（如保持输入输出同尺寸）。

(2)步幅：卷积核移动步长，减少输出尺寸（下采样）。

4 多通道机制:

(1)多输入通道：每个输入通道对应独立卷积核，结果求和。

(2)多输出通道：每组卷积核生成一个输出通道，扩展特征维度。

(3)1×1卷积：用于通道间信息融合，等效于全连接层。

5 汇聚层（池化）:

(1)最大池化：提取局部最显著特征，增强平移鲁棒性。

(2)平均池化：平滑特征响应，减少背景噪声。

6 LeNet实战:

(1)网络结构：卷积层→池化层→卷积层→池化层→全连接层。

(2)训练技巧：使用交叉熵损失和SGD优化器，应用于手写数字识别。

# 7.第七章：现代卷积神经网络

---

1 AlexNet:

(1)关键创新：ReLU激活函数、Dropout正则化、多GPU训练。

(2)ImageNet表现：2012年ImageNet冠军，Top-5错误率15.3%。

2 VGG:

(1)核心思想：重复使用简单块（3×3卷积+池化）构建深层网络。

(2)VGG块：多个3×3卷积堆叠，增大感受野（等效于更大卷积核）。

3 NiN（网络中的网络）:

(1)1×1卷积应用：替代全连接层，减少参数量。

(2)全局平均池化：直接生成类别概率，避免全连接层过拟合。

4 GoogLeNet:

(1)Inception块：并行多尺度卷积（1×1, 3×3, 5×5）融合特征。

(2)辅助分类器：中间层添加分类损失，缓解梯度消失。

5 批量规范化:

(1)原理：标准化层输入，加速训练并提升泛化能力。

(2)效果：允许使用更大学习率，减少对初始化的敏感度。

6 ResNet（残差网络）:

(1)残差块：引入跳跃连接（恒等映射），解决深层网络退化问题。

(2)残差学习：拟合目标变为F(x) = H(x) - x，而非直接拟合H(x)。

7 DenseNet（稠密连接）:

(1)稠密块：每层输入来自前面所有层的输出，特征重用最大化。

(2)过渡层：控制特征图数量，连接不同稠密块。

# 8.第八章：循环神经网络

---

1 序列模型基础:

(1)自回归模型：使用历史数据预测未来（如时间序列预测）。

(2)马尔可夫假设：当前状态仅依赖前τ个时间步。

2 文本预处理:

(1)词元化：分词（如BPE算法）或字符级切分。

(2)词表构建：建立词元到索引的映射，处理低频词（如<unk>）。

3 语言模型:

(1)n元语法：基于n-1阶马尔可夫假设的统计模型（如bigram）。

(2)长序列处理：随机采样或顺序分区生成训练样本。

4 RNN核心机制:

(1)隐状态：压缩历史信息的向量，在时间步间传递。

(2)循环层计算：hₜ = f(hₜ₋₁, xₜ)，如tanh(Wₕₕhₜ₋₁ + Wₕₓxₜ)。

(3)困惑度：评价语言模型性能，低困惑度表示预测更准。

5 RNN实现:

(1)独热编码：将词元转换为稀疏向量。

(2)梯度裁剪：防止梯度爆炸，限制梯度范数（如torch.nn.utils.clip_grad_norm_）。

# 9.第九章：现代循环神经网络

---

1 GRU（门控循环单元）:

（1）重置门：控制历史信息遗忘程度。

（2）更新门：平衡新输入与历史信息的比例。

2 LSTM（长短期记忆）:

(1)记忆元：存储长期信息的水平线。

(2)三门机制：输入门：控制新信息写入;遗忘门：控制历史信息遗忘;输出门：控制信息读出

3 深度RNN:

(1)堆叠结构：多个循环层堆叠，提取更高阶时序特征。

(2)跳跃连接：缓解深层网络优化困难。

4 双向RNN:

(1)上下文捕捉：同时利用过去和未来信息（如完形填空任务）。

(2)限制：不能用于实时预测（需完整序列输入）。

5 编码器-解码器架构:

(1)编码器：将输入序列压缩为上下文向量（如LSTM最后隐状态）。

(2)解码器：基于上下文向量生成输出序列（如机器翻译）。

6 Seq2Seq模型:

(1)教师强制：训练时将真实标签作为解码器输入，加速收敛。

(2)BLEU评分：评价生成文本与参考文本的相似度。

7 束搜索:

(1)贪心搜索缺点：局部最优导致全局次优。

(2)束搜索：保留Top-k候选序列，平衡质量与计算开销。

# 10.第十章：注意力机制

---

1.查询、建、值：

（1）查询：意志搜索。

（2）键：非意志线索。

（3)值：感觉输入。

2.注意力可视化：

（1）定义show——heatmaps函数，输入matrices的形状是 （要显示的行数，要显示的列数，查询的数目，键的数目）。

3.Nadaraya-Watson核回归模型：

（1）生成数据集 ϵ, （2）平均汇聚 （3）非注意力参数汇聚 （4）带参数注意力汇聚 （5）批量矩阵乘法 （6）定义模型 （7）训练

4.注意力评：得到注意力权重，最后输出这些注意力权重的值的加权和。

（1）掩蔽softmax操作：softmax操作用于输出一个概率分布作为注意力权重，并非所有的值都应该被纳入到注意力汇聚中。所以要进行掩蔽softmax。使用masked_softmax函数。

（2）加性注意力：一种评分函数，当查询和键是不同长度的矢量时，使用

（3）缩放点积注意力：使用点积可以得到计算效率更高的评分函数， 但是点积操作要求查询和键具有相同的长度。进行点积操作时令查询的特征维度与键的特征维度大小相同。

5.Bahdanau 注意力：没有严格单向对齐限制的 可微注意力模型

（1）模型：![模型图片](/image/25.png)

（2）定义注意力解码器：重新定义解码器编码器：

在所有时间步的最终层隐状态，将作为注意力的键和值；

上一时间步的编码器全层隐状态，将作为初始化解码器的隐状态；

编码器有效长度（排除在注意力池中填充词元）。

6.多头注意力：将这n个注意力汇聚的输出拼接在一起， 并且通过另一个可以学习的线性投影进行变换， 以产生最终输出。数
（1）实现：选择缩放点积注意力作为每一个注意力头。MultiHeadAttention类将使用transpose_output函数、transpose_qkv函数两个转置函数。

7.自注意力和位置编码：每个查询都会关注所有的键－值对并生成一个注意力输出。 由于查询、键和值来自同一组输入，因此被称为 自注意力。

（1）自注意力：输出与输入的张量形状相同。

（2)比较卷积神经网络、循环神经网络和自注意力:![模型图片](/image/26.png)

（3）位置编码：来注入绝对的或相对的位置信息。 位置编码可以通过学习得到也可以直接固定得到。 

（4） 绝对位置信息：一个确定的数有他相应的信息

（5）相对位置信息：基于相应的绝对位置信息，进行位置转移。

8.Transformer：Transformer模型完全基于注意力机制，没有任何卷积层或循环神经网络层。

（1）模型：![模型图片](/image/27.png)

（2） 基于位置的前馈网络：对序列中的所有位置的表示进行变换时使用的是同一个多层感知机（MLP），这就是称前馈网络是基于位置的（positionwise）的原因。

（3）残差连接和层规范化：层规范化和批量规范化的目标相同，但层规范化是基于特征维度进行规范化，使用残差连接和层规范化来实现AddNorm类。

（4）编码器：多头自注意力和基于位置的前馈网络

（5）解码器：由多个相同的层组成，每个层包含了三个子层：解码器自注意力、“编码器-解码器”注意力和基于位置的前馈网络。

# 11.第十一章：优化算法

---

1.优化和深度学习：在优化中，损失函数通常被称为优化问题的目标函数。

（1）优化的目标：减少泛化误差，防止过拟合。

（2）深度学习中的优化挑战：大多数目标函数都很复杂，没有解析解。相反，我们必须使用数值优化算法。近似该函数的局部最小值和全局最小值，鞍点（saddle point）是指函数的所有梯度都消失但既不是全局最小值也不是局部最小值的任何位置。可能遇到的最隐蔽问题是梯度消失（加入激活函数解决这一问题）。

2.凸性：凸性（convexity）在优化算法的设计中起到至关重要的作用， 这主要是由于在这种情况下对算法进行分析和测试要容易。

（1）凸集：凸集（convex set）是凸性的基础。两个凸集的交集是凸的。两个凸集的并集不一定是凸的。

（2）凸函数：余弦函数为非凸的，而抛物线函数和指数函数为凸的。

（3）詹森不等式：用一个较简单的表达式约束一个较复杂的表达式。

（4） 局部极小值是全局极小值，凸函数的下水平集是凸的

（5）约束：凸优化的一个很好的特性是能够让我们有效地处理约束（constraints）， 拉格朗日函数：一种至少近似地满足约束优化问题的方法是采用拉格朗日函数。添加惩罚是确保近似满足约束的一种好方法。 

（6）投影：满足约束条件的另一种策略是投影（projections）。

3.梯度下降：预处理（preconditioning）是梯度下降中的一种常用技术， 还被沿用到更高级的算法中。

（1）一维梯度下降:1.学习率：决定目标函数能否收敛到局部最小值，以及何时收敛到最小值。2.局部最小值：据我们选择的学习率，我们最终可能只会得到许多解的一个。

（2）多元梯度下降：需要两个辅助函数： 第一个是update函数，并将其应用于初始值20次； 第二个函数会显示轨迹

（3）自适应方法：1.牛顿法：最终将除以Hessian。2.收敛性分析 3.预处理：回避了计算整个Hessian，而只计算“对角线”项。4.梯度下降和线搜索：解决超过目标或进展不足。

4.随机梯度下降：梯度下降中每次迭代的梯度下降计算代价将较高，随机梯度是对完成梯度的无偏估计。

（1）动态学习率：用与时间相关的学习率增加了控制优化算法收敛的复杂性。使得参数的方差大大减少。

（2）凸目标的收敛性分析

（3）随机梯度和有限样本：重复采用训练数据集的时候，会以不同的随机顺序遍历它

（4）随机梯度下降的最优性保证在非凸情况下一般不可用，因为需要检查的局部最小值的数量可能是指数级的。

5.小批量随机梯度下降:使用完整数据集来计算梯度并更新参数和一次处理一个训练样本来取得进展的折中方案。

（1） 向量化和缓存：每次执行代码时，Python解释器都会向深度学习框架发送一个命令，要求将其插入到计算图中并在调度过程中处理它。 这样的额外开销可能是非常不利的。 总而言之，最好用向量化。

（2）小批量：原因是处理单个观测值需要执行许多单一矩阵-矢量（甚至矢量-矢量）乘法，这耗费相当大，而且对应深度学习框架也要巨大的开销。 这既适用于计算梯度以更新参数时，也适用于用神经网络预测。

6.动量法:动态调整学习率

（1）泄漏平均值 （2）条件不佳的问题 （3）有效样本权重：选取步长来控制学习率

（4）理论分析：1.二次凸函数：带有和带有不凸二次函数动量的梯度下降，可以分解为朝二次矩阵特征向量方向坐标顺序的优化。2.标量函数：重写更新方程

7.AdaGrad算法

（1）稀疏特征和学习率：学习率要么对于常见特征而言降低太慢，要么对于不常见特征而言降低太快。

（2）预处理：使用一个代理来表示黑塞矩阵（Hessian）的对角线，既相对易于计算又高效。

（3）算法：如果优化问题的结构相当不均匀，AdaGrad算法可以帮助缓解扭曲。使用变量来累加过去的梯度方差。AdaGrad算法对于稀疏特征特别有效，在此情况下由于不常出现的问题，学习率需要更慢地降低。

8.RMSProp算法

（1）算法：RMSProp算法与Adagrad算法非常相似，因为两者都使用梯度的平方来缩放系数。RMSProp算法与动量法都使用泄漏平均值。但是，RMSProp算法使用该技术来调整按系数顺序的预处理器。在实验中，学习率需要由实验者调度。

9.Adadelta

（1）Adadelta算法：Adadelta没有学习率参数。相反，它使用参数本身的变化率来调整学习率。Adadelta需要两个状态变量来存储梯度的二阶导数和参数的变化。Adadelta使用泄漏的平均值来保持对适当统计数据的运行估计。

10.Adam算法：将所有这些技术汇总到一个高效的学习算法中

（1）算法：它使用指数加权移动平均值来估算梯度的动量和二次矩，即它使用状态变量，首先，动量和规模在状态变量中清晰可见， 它们相当独特的定义使移除偏项（这可以通过稍微不同的初始化和更新条件来修正）。 其次，RMSProp算法中两项的组合都非常简单。 最后，明确的学习率使能够控制步长来解决收敛问题。

11.学习率调度器：不同的调度策略对准确性的影响，

（1）学习率调度器：在每个迭代轮数（甚至在每个小批量）之后向下调整学习率。定义一个调度器。 当调用更新次数时，它将返回学习率的适当值。

（2）策略：多项式衰减和分段常数表。1.单因子调度器：：多项式衰减的一种替代方案是乘法衰减。2.多因子调度器：保持学习率为一组分段的常量，并且不时地按给定的参数对学习率做乘法衰减。3.余弦调度器：一个类似于余弦的调度。在某些计算机视觉问题中很受欢迎。4.预热：选择一个足够小的学习率， 从而防止一开始发散，然而这样进展太缓慢。 另一方面，较高的学习率最初就会导致发散。
：

